---
title: "JMJPFU-DagSentiment-Experimentation"
output: html_notebook
---

# JMJPFU
### 14-May-2017

This is the notebook for doing experiments

```{r}
library(rvest)
library(dplyr)
library(tm)
library(ngram)
library(stringr)
library(wordcloud)
```

### Importing data set

```{r}
hotText <- Hotel_reviews[1,1]

paste(hotText)
```

### Step 1 : Let us now create some aspect Dictionaries

```{r}
## Aspect Dictionary: Food

foodDict <- list()

foodDict <- append(foodDict,"food")
foodDict <- append(foodDict,c("dishes","icecream","fish"))

foodDict <- unlist(foodDict)

########## Aspect Dictionary : Ambience

ambDict <- c("ambience","atmosphere")

########## Aspect Dictionary : Service

serDict <- c("service","waiter")

########## Aspect Pricing :

pricDict <- c("price","pricing","cost","expense")

```

### Creating sentiment dictionaries

```{r}
# Creating a positive dictionary
postDic <- c("consistent","good","fun","fast","consistently")

# Creating a negative dictionary

negDic <- c("bad","nonsense","no","exorbitant")

# Creating a polarity changing dictionary

polDic <- c("no","not","never")


```






### Step 1 : Splitting as per different lines

```{r}

# Doing the split sentence by sentence

textSplit <- str_split(hotText,"[.]")

# Unlisting the split so that it is a list of sentences

textSplit <- unlist(textSplit)

# Finding the sentence length

sentLength <- length(textSplit)

# Looping over the list of sentences

for(i in 1: sentLength){
  
  textSamp <- textSplit[i]
  
  # Let us clean up the text
  
  textSamp <- cleansamp(textSamp)
  
  # Converting to a corpus
  
  textCorp <- Corpus(VectorSource(textSamp))
  
  # Let us remove the stop words
  
  textCorp <- tm_map(textCorp,removeWords,c("the","was","i","at","a","when","is","and","has","for","of","are","to","an","it","in","be","if","on","since","as","had","so","he","him","me","her","she","its","that","its","been","he","there"))
  
  textCorp <- tm_map(textCorp,stripWhitespace)
  
 # Getting back the clean text
  
  textClean <- textCorp1[[1]]$content
  
  textClean <- stripWhitespace(textClean)
  
 # Split the text again based on the spaces 
  
  cleantxtSplit <- str_split(textClean," ")
  
  # Unlisting the text again
  
  cleantxtSplit <- unlist(cleantxtSplit)
  
  ############################### DAG Extraction Layer ######################
  
  # Getting polarity of each aspect
  
  foodPolarity <- recScanner(cleantxtSplit,"food")$dagStore 
  foodPos <- recScanner(cleantxtSplit,"food")$PCount
  foodNeg <- recScanner(cleantxtSplit,"food")$Ncount
  
  ambPolarity <- recScanner(cleantxtSplit,"ambience")$dagStore
  ambPos <- recScanner(cleantxtSplit,"ambience")$PCount
  ambNeg <- recScanner(cleantxtSplit,"ambience")$Ncount
  
  serPolarity <- recScanner(cleantxtSplit,"service")$dagStore
  serPos <- recScanner(cleantxtSplit,"service")$PCount
  serNeg <- recScanner(cleantxtSplit,"service")$Ncount
  
  pricePolarity <- recScanner(cleantxtSplit,"price")$dagStore
  pricePos <- recScanner(cleantxtSplit,"price")$PCount
  priceNeg <- recScanner(cleantxtSplit,"price")$Ncount
  
} # End of the first for loop



```
# JMJPFU
### 15-May-2017

1. Created a function for the aspect - sentiment search and the DAG updater

# JMJPFU
### 18-May-2017

1. Created a polarity changing words like (no, not, never). Applying before any negative or positive words changes its polarity
2. Created a counter of polarity words

# Tomorrow

1. The number of negative words needs a re-look. The count is not working properly
2. Create DAG updater for all sentiments
