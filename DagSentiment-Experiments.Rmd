---
title: "JMJPFU-DagSentiment-Experimentation"
output: html_notebook
---

# JMJPFU
### 14-May-2017

This is the notebook for doing experiments

```{r}
library(rvest)
library(dplyr)
library(tm)
library(ngram)
library(stringr)
library(wordcloud)
```

### Importing data set

```{r}
hotText <- Hotel_reviews[1,1]

paste(hotText)
```

### Step 1 : Let us now create some aspect Dictionaries

```{r}
## Aspect Dictionary: Food

foodDict <- list()

foodDict <- append(foodDict,"food")
foodDict <- append(foodDict,c("dishes","icecream","fish"))

foodDict <- unlist(foodDict)

########## Aspect Dictionary : Ambience

ambDict <- c("ambience","atmosphere")

########## Aspect Dictionary : Service

serDict <- c("service","waiter")

########## Aspect Pricing :

pricDict <- c("price","pricing","cost","expense")

```

### Creating sentiment dictionaries

```{r}
# Creating a positive dictionary
postDic <- c("consistent","good","fun","fast","consistently")

# Creating a negative dictionary

negDic <- c("bad","nonsense","no","exorbitant")


```






### Step 1 : Splitting as per different lines

```{r}

# Doing the split sentence by sentence

textSplit <- str_split(hotText,"[.]")

# Unlisting the split so that it is a list of sentences

textSplit <- unlist(textSplit)

# Finding the sentence length

sentLength <- length(textSplit)

# Looping over the list of sentences

for(i in 1: sentLength){
  
  textSamp <- textSplit[i]
  
  # Let us clean up the text
  
  textSamp <- cleansamp(textSamp)
  
  # Converting to a corpus
  
  textCorp <- Corpus(VectorSource(textSamp))
  
  # Let us remove the stop words
  
  textCorp <- tm_map(textCorp,removeWords,c("the","was","i","at","a","when","is","and","has","for","of","are","to","an","it","in","be","if","on","since","as","had","so","he","him","me","her","she","its","that","its","been","he","there"))
  
  textCorp <- tm_map(textCorp,stripWhitespace)
  
 # Getting back the clean text
  
  textClean <- textCorp1[[1]]$content
  
  textClean <- stripWhitespace(textClean)
  
 # Split the text again based on the spaces 
  
  cleantxtSplit <- str_split(textClean," ")
  
  # Unlisting the text again
  
  cleantxtSplit <- unlist(cleantxtSplit)
  
  ############################### DAG Extraction Layer ######################
  
  # Getting polarity of each aspect
  
  foodPolarity <- recScanner(cleantxtSplit,"food") 
  ambPolarity <- recScanner(cleantxtSplit,"ambience")
  serPolarity <- recScanner(cleantxtSplit,"service")
  pricePolarity <- recScanner(cleantxtSplit,"price")
  
} # End of the first for loop



```
# JMJPFU
### 15-May-2017

1. Created a function for the aspect - sentiment search and the DAG updater

# Tomorrow

1. Create a polarity changing words like (no, not, never). Applying before any negative or positive words changes its polarity

2. Update the DAG score in a data frame. Based on the number of positive words create a feature for the update
3. Create DAG updater for all sentiments
